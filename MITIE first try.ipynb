{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "mitie_path = '/home/tsdaemon/sources/MITIE/'\n",
    "sys.path.append(mitie_path + 'mitielib')\n",
    "\n",
    "\n",
    "from mitie import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading NER model...\n",
      "\n",
      "Tags output by this NER model: ['PERSON', 'LOCATION', 'ORGANIZATION', 'MISC']\n",
      "Tokenized input: ['A', 'Pegasus', 'Airlines', 'plane', 'landed', 'at', 'an', 'Istanbul', 'airport', 'Friday', 'after', 'a', 'passenger', '\"', 'said', 'that', 'there', 'was', 'a', 'bomb', 'on', 'board', '\"', 'and', 'wanted', 'the', 'plane', 'to', 'land', 'in', 'Sochi', ',', 'Russia', ',', 'the', 'site', 'of', 'the', 'Winter', 'Olympics', ',', 'said', 'officials', 'with', 'Turkey', \"'s\", 'Transportation', 'Ministry', '.', 'Meredith', 'Vieira', 'will', 'become', 'the', 'first', 'woman', 'to', 'host', 'Olympics', 'primetime', 'coverage', 'on', 'her', 'own', 'when', 'she', 'fills', 'on', 'Friday', 'night', 'for', 'the', 'ailing', 'Bob', 'Costas', ',', 'who', 'is', 'battling', 'a', 'continuing', 'eye', 'infection', '.', '\"', 'It', \"'s\", 'an', 'honor', 'to', 'fill', 'in', 'for', 'him', ',', '\"', 'Vieira', 'said', 'on', 'TODAY', 'Friday', '.', '\"', 'You', 'think', 'about', 'the', 'Olympics', ',', 'and', 'you', 'think', 'the', 'athletes', 'and', 'then', 'Bob', 'Costas', '.', '\"', '\"', 'Bob', \"'s\", 'eye', 'issue', 'has', 'improved', 'but', 'he', \"'s\", 'not', 'quite', 'ready', 'to', 'do', 'the', 'show', ',', '\"', 'NBC', 'Olympics', 'Executive', 'Producer', 'Jim', 'Bell', 'told', 'TODAY', '.', 'com', 'from', 'Sochi', 'on', 'Thursday', '.', 'From', 'wikipedia', 'we', 'learn', 'that', 'Josiah', 'Franklin', \"'s\", 'son', ',', 'Benjamin', 'Franklin', 'was', 'born', 'in', 'Boston', '.', 'Since', 'wikipedia', 'allows', 'anyone', 'to', 'edit', 'it', ',', 'you', 'could', 'change', 'the', 'entry', 'to', 'say', 'that', 'Philadelphia', 'is', 'the', 'birthplace', 'of', 'Benjamin', 'Franklin', '.', 'However', ',', 'that', 'would', 'be', 'a', 'bad', 'edit', 'since', 'Benjamin', 'Franklin', 'was', 'definitely', 'born', 'in', 'Boston', '.']\n"
     ]
    }
   ],
   "source": [
    "print \"loading NER model...\"\n",
    "ner = named_entity_extractor(mitie_path + 'MITIE-models/english/ner_model.dat')\n",
    "print \"\\nTags output by this NER model:\", ner.get_possible_ner_tags()\n",
    "\n",
    "# Load a text file and convert it into a list of words.  \n",
    "tokens = tokenize(load_entire_file(mitie_path + 'sample_text.txt'))\n",
    "print \"Tokenized input:\", tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entities found: [(xrange(1, 3), 'ORGANIZATION', 0.5662716633696536), (xrange(7, 8), 'LOCATION', 1.3961989551275185), (xrange(30, 31), 'LOCATION', 1.5617225956091672), (xrange(32, 33), 'LOCATION', 1.2932414954225302), (xrange(44, 45), 'LOCATION', 1.1356116407121106), (xrange(46, 48), 'ORGANIZATION', 0.9876062161909271), (xrange(49, 51), 'PERSON', 1.0676552859044377), (xrange(58, 59), 'MISC', 1.190955875245744), (xrange(73, 75), 'PERSON', 0.8800389619396217), (xrange(96, 97), 'PERSON', 1.2623335607783588), (xrange(107, 108), 'MISC', 0.7035980014268862), (xrange(116, 118), 'PERSON', 0.9773891906703803), (xrange(121, 122), 'PERSON', 1.3240247462100005), (xrange(139, 141), 'ORGANIZATION', 0.73640824406934), (xrange(143, 145), 'PERSON', 1.579659392025735), (xrange(150, 151), 'LOCATION', 0.9852057723609379), (xrange(159, 161), 'PERSON', 1.4906579997085982), (xrange(164, 166), 'PERSON', 1.5127579847728057), (xrange(169, 170), 'LOCATION', 1.3205684671616802), (xrange(187, 188), 'LOCATION', 0.7658788239592431), (xrange(192, 194), 'PERSON', 0.9780207141954799), (xrange(204, 206), 'PERSON', 1.205128043686874), (xrange(210, 211), 'LOCATION', 1.1432445561981999)]\n",
      "\n",
      "Number of entities detected: 23\n"
     ]
    }
   ],
   "source": [
    "entities = ner.extract_entities(tokens)\n",
    "print \"\\nEntities found:\", entities\n",
    "print \"\\nNumber of entities detected:\", len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score: 0.566: ORGANIZATION: Pegasus Airlines\n",
      "   Score: 1.396: LOCATION: Istanbul\n",
      "   Score: 1.562: LOCATION: Sochi\n",
      "   Score: 1.293: LOCATION: Russia\n",
      "   Score: 1.136: LOCATION: Turkey\n",
      "   Score: 0.988: ORGANIZATION: Transportation Ministry\n",
      "   Score: 1.068: PERSON: Meredith Vieira\n",
      "   Score: 1.191: MISC: Olympics\n",
      "   Score: 0.880: PERSON: Bob Costas\n",
      "   Score: 1.262: PERSON: Vieira\n",
      "   Score: 0.704: MISC: Olympics\n",
      "   Score: 0.977: PERSON: Bob Costas\n",
      "   Score: 1.324: PERSON: Bob\n",
      "   Score: 0.736: ORGANIZATION: NBC Olympics\n",
      "   Score: 1.580: PERSON: Jim Bell\n",
      "   Score: 0.985: LOCATION: Sochi\n",
      "   Score: 1.491: PERSON: Josiah Franklin\n",
      "   Score: 1.513: PERSON: Benjamin Franklin\n",
      "   Score: 1.321: LOCATION: Boston\n",
      "   Score: 0.766: LOCATION: Philadelphia\n",
      "   Score: 0.978: PERSON: Benjamin Franklin\n",
      "   Score: 1.205: PERSON: Benjamin Franklin\n",
      "   Score: 1.143: LOCATION: Boston\n"
     ]
    }
   ],
   "source": [
    "# entities is a list of tuples, each containing an xrange that indicates which\n",
    "# tokens are part of the entity, the entity tag, and an associate score.  The\n",
    "# entities are also listed in the order they appear in the input text file.\n",
    "# Here we just print the score, tag, and text for each entity to the screen.\n",
    "# The larger the score the more confident MITIE is in its prediction.\n",
    "for e in entities:\n",
    "    range = e[0]\n",
    "    tag = e[1]\n",
    "    score = e[2]\n",
    "    score_text = \"{:0.3f}\".format(score)\n",
    "    entity_text = \" \".join(tokens[i] for i in range)\n",
    "    print \"   Score: \" + score_text + \": \" + tag + \": \" + entity_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's run one of MITIE's binary relation detectors.  MITIE comes with a\n",
    "# bunch of different types of relation detector and includes tools allowing you\n",
    "# to train new detectors.  However, here we simply use one, the \"person born in\n",
    "# place\" relation detector.\n",
    "rel_detector = binary_relation_detector(mitie_path + \"MITIE-models/english/binary_relations/rel_classifier_people.person.place_of_birth.svm\")\n",
    "\n",
    "# First, let's make a list of neighboring entities.  Once we have this list we\n",
    "# will ask the relation detector if any of these entity pairs is an example of\n",
    "# the \"person born in place\" relation. \n",
    "neighboring_entities = [(entities[i][0], entities[i+1][0]) for i in xrange(len(entities)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamin Franklin BORN_IN Boston\n",
      "Benjamin Franklin BORN_IN Boston\n",
      "Benjamin Franklin BORN_IN Philadelphia\n"
     ]
    }
   ],
   "source": [
    "# Also swap the entities and add those in as well.  We do this because \"person\n",
    "# born in place\" mentions can appear in the text in as \"place is birthplace of\n",
    "# person\".  So we must consider both possible orderings of the arguments.\n",
    "neighboring_entities += [(r,l) for (l,r) in neighboring_entities]\n",
    "\n",
    "# Now that we have our list, let's check each entity pair and see which one the\n",
    "# detector selects.\n",
    "for person, place in neighboring_entities:\n",
    "    # Detection has two steps in MITIE. First, you convert a pair of entities\n",
    "    # into a special representation.\n",
    "    rel = ner.extract_binary_relation(tokens, person, place)\n",
    "    # Then you ask the detector to classify that pair of entities.  If the\n",
    "    # score value is > 0 then it is saying that it has found a relation.  The\n",
    "    # larger the score the more confident it is.  Finally, the reason we do\n",
    "    # detection in two parts is so you can reuse the intermediate rel in many\n",
    "    # calls to different relation detectors without needing to redo the\n",
    "    # processing done in extract_binary_relation().\n",
    "    score = rel_detector(rel)\n",
    "    # Print out any matching relations.\n",
    "    if (score > 0):\n",
    "        person_text     = \" \".join(tokens[i] for i in person)\n",
    "        birthplace_text = \" \".join(tokens[i] for i in place)\n",
    "        print person_text, \"BORN_IN\", birthplace_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top most common relations:\n",
      "2 relations claiming Benjamin Franklin was born in Boston\n",
      "1 relations claiming Benjamin Franklin was born in Philadelphia\n"
     ]
    }
   ],
   "source": [
    "# The code above shows the basic details of MITIE's relation detection API.\n",
    "# However, it is important to note that real world data is noisy any confusing.\n",
    "# Not all detected relations will be correct.  Therefore, it's important to\n",
    "# aggregate many relation detections together to get the best signal out of\n",
    "# your data.  A good way to do this is to pick an entity you are in interested\n",
    "# in (e.g. Benjamin Franklin) and then find all the relations that mention him\n",
    "# and order them by most frequent to least frequent.  We show how to do this in\n",
    "# the code below.\n",
    "query = \"Benjamin Franklin\"\n",
    "hits = defaultdict(int)\n",
    "\n",
    "for person, place in neighboring_entities:\n",
    "    rel = ner.extract_binary_relation(tokens, person, place)\n",
    "    score = rel_detector(rel)\n",
    "    if (score > 0):\n",
    "        person_text     = \" \".join(tokens[i] for i in person)\n",
    "        birthplace_text = \" \".join(tokens[i] for i in place)\n",
    "        if (person_text == query):\n",
    "            hits[birthplace_text] += 1\n",
    "\n",
    "print \"\\nTop most common relations:\"\n",
    "for place, count in sorted(hits.iteritems(), key=lambda x:x[1], reverse=True):\n",
    "    print count, \"relations claiming\", query, \"was born in\", place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
